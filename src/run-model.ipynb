{
 "cells": [
  {
   "cell_type": "code",
   "id": "d6b49eff-d8c9-4ed3-8b26-b994ccd43bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T06:22:17.698436Z",
     "start_time": "2024-11-24T06:22:11.222684Z"
    }
   },
   "source": [
    "%env BNB_CUDA_VERSION=125\n",
    "from modeling_quiet_star_mistral import QuietMistralForCausalLM\n",
    "import torch"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BNB_CUDA_VERSION=125\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "841248f8-8028-4f12-868c-ff24385dc012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T06:22:17.913952Z",
     "start_time": "2024-11-24T06:22:17.909386Z"
    }
   },
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a0eb46fa-4d56-4264-b4a6-7365fb0d98de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T06:22:18.798579Z",
     "start_time": "2024-11-24T06:22:18.279344Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('ezelikman/quietstar-8-ahead')\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "ffeb2a67-37b1-4e45-a1c8-79e814ae03b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T06:22:47.119921Z",
     "start_time": "2024-11-24T06:22:19.090871Z"
    }
   },
   "source": [
    "n_ahead = 42\n",
    "n_ahead_talk = 1\n",
    "\n",
    "model = QuietMistralForCausalLM.from_pretrained(\n",
    "\t'ezelikman/quietstar-8-ahead',\n",
    "        #  load_in_8bit=True,\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        max_thoughts=n_ahead + n_ahead_talk + 1,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "QuietMistralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b23c0eaaf64f4529a5802fdb2b3ffc58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "ef78057a-a0b0-4bca-bc43-141f4bd6cd2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-24T06:22:51.365457Z",
     "start_time": "2024-11-24T06:22:51.359089Z"
    }
   },
   "source": [
    "model.use_end_thought_token = True\n",
    "model.tokenizer = tokenizer\n",
    "model.use_start_thought_token = True\n",
    "model.wandb_enabled = True\n",
    "model.n_ahead = n_ahead\n",
    "model.kill_after = 100\n",
    "model.rm_initialized = True\n",
    "model.use_policy_loss = False\n",
    "model.training = False\n",
    "# model.eval()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T06:27:09.848514Z",
     "start_time": "2024-11-24T06:27:09.456809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from inspect import getmembers\n",
    "print(getmembers(model))"
   ],
   "id": "c7cfcfd5c01652f5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5006: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('T_destination', ~T_destination), ('__annotations__', {'dump_patches': <class 'bool'>, '_version': <class 'int'>, 'training': <class 'bool'>, '_parameters': typing.Dict[str, typing.Optional[torch.nn.parameter.Parameter]], '_buffers': typing.Dict[str, typing.Optional[torch.Tensor]], '_non_persistent_buffers_set': typing.Set[str], '_backward_pre_hooks': typing.Dict[int, typing.Callable], '_backward_hooks': typing.Dict[int, typing.Callable], '_is_full_backward_hook': typing.Optional[bool], '_forward_hooks': typing.Dict[int, typing.Callable], '_forward_hooks_with_kwargs': typing.Dict[int, bool], '_forward_hooks_always_called': typing.Dict[int, bool], '_forward_pre_hooks': typing.Dict[int, typing.Callable], '_forward_pre_hooks_with_kwargs': typing.Dict[int, bool], '_state_dict_hooks': typing.Dict[int, typing.Callable], '_load_state_dict_pre_hooks': typing.Dict[int, typing.Callable], '_state_dict_pre_hooks': typing.Dict[int, typing.Callable], '_load_state_dict_post_hooks': typing.Dict[int, typing.Callable], '_modules': typing.Dict[str, typing.Optional[ForwardRef('Module')]], 'call_super_init': <class 'bool'>, '_compiled_call_impl': typing.Optional[typing.Callable], 'forward': typing.Callable[..., typing.Any], '__call__': typing.Callable[..., typing.Any]}), ('__call__', <bound method Module._wrapped_call_impl of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('__class__', <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>), ('__delattr__', <bound method Module.__delattr__ of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('__dict__', {'training': False, '_parameters': {'start_embedding': Parameter containing:\n",
      "tensor([[-5.9605e-05,  2.0623e-05, -4.8399e-05,  ...,  3.6955e-05,\n",
      "         -2.3007e-05,  1.5855e-05],\n",
      "        [-1.5125e+01, -1.5125e+01, -1.5125e+01,  ..., -1.5125e+01,\n",
      "         -1.5125e+01, -1.5125e+01]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True), 'end_embedding': Parameter containing:\n",
      "tensor([[-1.8716e-05, -2.1458e-06,  1.4842e-05,  ...,  4.3154e-05,\n",
      "         -5.0545e-05,  2.6107e-05],\n",
      "        [-1.5125e+01, -1.5125e+01, -1.5125e+01,  ..., -1.5125e+01,\n",
      "         -1.5125e+01, -1.5125e+01]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)}, '_buffers': {}, '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_hooks_always_called': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': {'model': MistralModel(\n",
      "  (embed_tokens): Embedding(32002, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x MistralDecoderLayer(\n",
      "      (self_attn): MistralSdpaAttention(\n",
      "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): MistralMLP(\n",
      "        (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "), 'lm_head': Linear(in_features=4096, out_features=32002, bias=False), 'talk_head': ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "  )\n",
      ")}, 'config': MistralConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"ezelikman/quietstar-8-ahead\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_thoughts\": 44,\n",
      "  \"merged_lm_and_talk_heads\": false,\n",
      "  \"merged_lm_and_think_heads\": true,\n",
      "  \"merged_talk_heads\": true,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_complex_talk_head\": true,\n",
      "  \"use_complex_think_head\": false,\n",
      "  \"use_concat_talk_head\": true,\n",
      "  \"use_shallow_talk\": false,\n",
      "  \"use_shallow_think\": true,\n",
      "  \"use_weighted_talk_head\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      ", 'name_or_path': 'ezelikman/quietstar-8-ahead', 'warnings_issued': {}, 'generation_config': GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      ", '_keep_in_fp32_modules': None, 'vocab_size': 32002, 'max_thoughts': 44, 'merged_lm_and_talk_heads': False, 'use_concat_talk_head': True, 'use_shallow_talk': False, 'use_complex_talk_head': True, 'use_weighted_talk_head': True, 'n_ahead': 42, 'n_ahead_talk': 1, 'n_passes': 1, 'n_tokens_print': 1, 'gradient_accumulation_steps': 1, 'training_steps': 0, 'tokenizer': LlamaTokenizerFast(name_or_path='ezelikman/quietstar-8-ahead', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<|endthought|>', '<|startthought|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<|endthought|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|startthought|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, 'start_token_id': None, 'end_token_id': None, 'rm_initialized': True, 'residual_talk_head': True, 'thought_init_std_scale': 0.01, 'final_only_mode': False, 'first_and_last_mode': True, 'first_only': False, 'original_loss_weight': 0.5, 'cumulative_residual': False, 'clever_residual': False, 'skip_residual': False, 'no_residual': True, 'optimize_lm_head_only_at_start': False, 'optimize_model_only_at_start': False, 'train_only_thinking_embedding': False, 'weighted_embeddings': False, 'use_start_thought_token': True, 'use_end_thought_token': True, 'initialize_thought_embedding_to_normal': False, 'initial_start_token': '---', 'initial_end_token': '---', 'output_logits_at_the_end': True, 'wandb_enabled': True, 'gumbel_temperature': 0.001, 'use_policy_loss': False, 'include_policy_loss': True, 'trice_mode': True, 'remove_negative_rewards': True, 'use_policy_loss_for_end_thought': True, 'base_original_mode': False, 'original_mode': False, 'thought_prefix': \"(Let's think step by step\", 'tokenized_thought_prefix': None, 'log_dict': defaultdict(<class 'int'>, {}), 'eval_log_dict': defaultdict(<class 'int'>, {}), 'print_final_only': True, 'loss_mean': <function loss_mean at 0x7dfd7b90b400>, 'all_rewards': [], 'all_unreduced_losses': [], 'kill_after': 100, 'policy_loss_beta': 1000000.0, 'embedding_scale': 100.0, 'reinforce_temperature': 3, 'base_loss_beta': 1, 'use_thought_prefix': False, 'use_reparam_for_thought_embeddings': False, 'use_upper_triangular': False, 'subtract_mean_reward': False, 'comparison_mode': False, 'gumbel_detach': True, 'eval_mode': False, 'is_quantized': True, 'quantization_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>, '_is_hf_initialized': True, '_old_forward': <bound method QuietMistralForCausalLM.forward of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>, '_hf_hook': AlignDevicesHook(execution_device=0, offload=False, io_same_device=True, offload_buffers=False, place_submodules=True, skip_keys='past_key_values'), 'forward': functools.partial(<function add_hook_to_module.<locals>.new_forward at 0x7dfd75349360>, QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")), 'to': <function Module.to at 0x7dfd4c9d3760>, 'cuda': <function Module.cuda at 0x7dfd4c9d37f0>, 'hf_device_map': {'': 0}, 'is_loaded_in_4bit': True, 'is_4bit_serializable': True, 'hf_quantizer': <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7dfd75360b50>}), ('__dir__', <bound method Module.__dir__ of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('__doc__', None), ('__eq__', <method-wrapper '__eq__' of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__format__', <built-in method __format__ of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__ge__', <method-wrapper '__ge__' of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__getattr__', <bound method Module.__getattr__ of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('__getattribute__', <method-wrapper '__getattribute__' of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__getstate__', <bound method Module.__getstate__ of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('__gt__', <method-wrapper '__gt__' of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__hash__', <method-wrapper '__hash__' of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__init__', <bound method QuietMistralForCausalLM.__init__ of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('__init_subclass__', <built-in method __init_subclass__ of type object at 0x5fe023c71930>), ('__le__', <method-wrapper '__le__' of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__lt__', <method-wrapper '__lt__' of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__module__', 'modeling_quiet_star_mistral'), ('__ne__', <method-wrapper '__ne__' of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__new__', <built-in method __new__ of type object at 0x5fe019176800>), ('__reduce__', <built-in method __reduce__ of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__reduce_ex__', <built-in method __reduce_ex__ of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__repr__', <bound method Module.__repr__ of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('__setattr__', <bound method Module.__setattr__ of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('__setstate__', <bound method Module.__setstate__ of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('__sizeof__', <built-in method __sizeof__ of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__str__', <method-wrapper '__str__' of QuietMistralForCausalLM object at 0x7dfd7b90ce20>), ('__subclasshook__', <built-in method __subclasshook__ of type object at 0x5fe023c71930>), ('__weakref__', None), ('_apply', <bound method Module._apply of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_assisted_decoding', <bound method GenerationMixin._assisted_decoding of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_auto_class', None), ('_autoset_attn_implementation', <bound method PreTrainedModel._autoset_attn_implementation of <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>>), ('_backward_compatibility_gradient_checkpointing', <bound method PreTrainedModel._backward_compatibility_gradient_checkpointing of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_backward_hooks', OrderedDict()), ('_backward_pre_hooks', OrderedDict()), ('_beam_search', <bound method GenerationMixin._beam_search of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_buffers', {}), ('_call_impl', <bound method Module._call_impl of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_check_and_enable_flash_attn_2', <bound method PreTrainedModel._check_and_enable_flash_attn_2 of <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>>), ('_check_and_enable_sdpa', <bound method PreTrainedModel._check_and_enable_sdpa of <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>>), ('_compiled_call_impl', None), ('_constrained_beam_search', <bound method GenerationMixin._constrained_beam_search of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_contrastive_search', <bound method GenerationMixin._contrastive_search of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_convert_head_mask_to_5d', <bound method ModuleUtilsMixin._convert_head_mask_to_5d of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_copy_lm_head_original_to_resized', <bound method PreTrainedModel._copy_lm_head_original_to_resized of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_create_repo', <bound method PushToHubMixin._create_repo of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_dispatch_accelerate_model', <bound method PeftAdapterMixin._dispatch_accelerate_model of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_dola_decoding', <bound method GenerationMixin._dola_decoding of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_expand_inputs_for_generation', <function GenerationMixin._expand_inputs_for_generation at 0x7dfe1b59dab0>), ('_extract_past_from_model_output', <bound method GenerationMixin._extract_past_from_model_output of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_forward_hooks', OrderedDict()), ('_forward_hooks_always_called', OrderedDict()), ('_forward_hooks_with_kwargs', OrderedDict()), ('_forward_pre_hooks', OrderedDict()), ('_forward_pre_hooks_with_kwargs', OrderedDict()), ('_from_config', <bound method PreTrainedModel._from_config of <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>>), ('_get_backward_hooks', <bound method Module._get_backward_hooks of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_backward_pre_hooks', <bound method Module._get_backward_pre_hooks of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_cache', <bound method GenerationMixin._get_cache of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_candidate_generator', <bound method GenerationMixin._get_candidate_generator of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_files_timestamps', <bound method PushToHubMixin._get_files_timestamps of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_initial_cache_position', <bound method GenerationMixin._get_initial_cache_position of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_logits_processor', <bound method GenerationMixin._get_logits_processor of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_name', <bound method Module._get_name of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_no_split_modules', <bound method PreTrainedModel._get_no_split_modules of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_resized_embeddings', <bound method PreTrainedModel._get_resized_embeddings of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_resized_lm_head', <bound method PreTrainedModel._get_resized_lm_head of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_get_stopping_criteria', <bound method GenerationMixin._get_stopping_criteria of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_group_beam_search', <bound method GenerationMixin._group_beam_search of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_has_unfinished_sequences', <bound method GenerationMixin._has_unfinished_sequences of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_hf_hook', AlignDevicesHook(execution_device=0, offload=False, io_same_device=True, offload_buffers=False, place_submodules=True, skip_keys='past_key_values')), ('_hf_peft_config_loaded', False), ('_hook_rss_memory_post_forward', <function ModuleUtilsMixin._hook_rss_memory_post_forward at 0x7dfe1a447490>), ('_hook_rss_memory_pre_forward', <function ModuleUtilsMixin._hook_rss_memory_pre_forward at 0x7dfe1a447400>), ('_init_added_embeddings_weights_with_mean', <bound method PreTrainedModel._init_added_embeddings_weights_with_mean of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_init_added_lm_head_bias_with_mean', <bound method PreTrainedModel._init_added_lm_head_bias_with_mean of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_init_added_lm_head_weights_with_mean', <bound method PreTrainedModel._init_added_lm_head_weights_with_mean of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_init_weights', <bound method MistralPreTrainedModel._init_weights of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_initialize_thought_tokens', <bound method QuietMistralForCausalLM._initialize_thought_tokens of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_initialize_weights', <bound method PreTrainedModel._initialize_weights of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_is_full_backward_hook', None), ('_is_hf_initialized', True), ('_is_quantized_training_enabled', True), ('_is_stateful', False), ('_keep_in_fp32_modules', None), ('_keys_to_ignore_on_load_missing', None), ('_keys_to_ignore_on_load_unexpected', None), ('_keys_to_ignore_on_save', None), ('_load_from_state_dict', <bound method Module._load_from_state_dict of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_load_pretrained_model', <bound method PreTrainedModel._load_pretrained_model of <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>>), ('_load_pretrained_model_low_mem', <function PreTrainedModel._load_pretrained_model_low_mem at 0x7dfe1a44d900>), ('_load_state_dict_post_hooks', OrderedDict()), ('_load_state_dict_pre_hooks', OrderedDict()), ('_maybe_initialize_input_ids_for_generation', <bound method GenerationMixin._maybe_initialize_input_ids_for_generation of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_maybe_warn_non_full_backward_hook', <bound method Module._maybe_warn_non_full_backward_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_merge_criteria_processor_list', <bound method GenerationMixin._merge_criteria_processor_list of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_modules', {'model': MistralModel(\n",
      "  (embed_tokens): Embedding(32002, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x MistralDecoderLayer(\n",
      "      (self_attn): MistralSdpaAttention(\n",
      "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): MistralMLP(\n",
      "        (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "), 'lm_head': Linear(in_features=4096, out_features=32002, bias=False), 'talk_head': ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "  )\n",
      ")}), ('_named_members', <bound method Module._named_members of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_no_split_modules', ['MistralDecoderLayer']), ('_non_persistent_buffers_set', set()), ('_old_forward', <bound method QuietMistralForCausalLM.forward of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_parameters', {'start_embedding': Parameter containing:\n",
      "tensor([[-5.9605e-05,  2.0623e-05, -4.8399e-05,  ...,  3.6955e-05,\n",
      "         -2.3007e-05,  1.5855e-05],\n",
      "        [-1.5125e+01, -1.5125e+01, -1.5125e+01,  ..., -1.5125e+01,\n",
      "         -1.5125e+01, -1.5125e+01]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True), 'end_embedding': Parameter containing:\n",
      "tensor([[-1.8716e-05, -2.1458e-06,  1.4842e-05,  ...,  4.3154e-05,\n",
      "         -5.0545e-05,  2.6107e-05],\n",
      "        [-1.5125e+01, -1.5125e+01, -1.5125e+01,  ..., -1.5125e+01,\n",
      "         -1.5125e+01, -1.5125e+01]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)}), ('_prepare_attention_mask_for_generation', <bound method GenerationMixin._prepare_attention_mask_for_generation of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_prepare_cache_for_generation', <bound method GenerationMixin._prepare_cache_for_generation of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_prepare_decoder_input_ids_for_generation', <bound method GenerationMixin._prepare_decoder_input_ids_for_generation of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_prepare_encoder_decoder_kwargs_for_generation', <bound method GenerationMixin._prepare_encoder_decoder_kwargs_for_generation of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_prepare_generated_length', <bound method GenerationMixin._prepare_generated_length of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_prepare_generation_config', <bound method GenerationMixin._prepare_generation_config of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_prepare_model_inputs', <bound method GenerationMixin._prepare_model_inputs of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_prepare_special_tokens', <bound method GenerationMixin._prepare_special_tokens of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_register_load_state_dict_pre_hook', <bound method Module._register_load_state_dict_pre_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_register_state_dict_hook', <bound method Module._register_state_dict_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_reorder_cache', <function QuietMistralForCausalLM._reorder_cache at 0x7dfd7b90bb50>), ('_replicate_for_data_parallel', <bound method Module._replicate_for_data_parallel of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_resize_token_embeddings', <bound method PreTrainedModel._resize_token_embeddings of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_sample', <bound method GenerationMixin._sample of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_save_to_state_dict', <bound method Module._save_to_state_dict of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_set_default_torch_dtype', <bound method PreTrainedModel._set_default_torch_dtype of <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>>), ('_set_gradient_checkpointing', <bound method PreTrainedModel._set_gradient_checkpointing of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_skip_keys_device_placement', 'past_key_values'), ('_slow_forward', <bound method Module._slow_forward of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_state_dict_hooks', OrderedDict()), ('_state_dict_pre_hooks', OrderedDict()), ('_supports_cache_class', True), ('_supports_default_dynamic_cache', <bound method GenerationMixin._supports_default_dynamic_cache of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_supports_flash_attn_2', True), ('_supports_num_logits_to_keep', <bound method GenerationMixin._supports_num_logits_to_keep of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_supports_quantized_cache', False), ('_supports_sdpa', True), ('_supports_static_cache', True), ('_temporary_reorder_cache', <bound method GenerationMixin._temporary_reorder_cache of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_tie_encoder_decoder_weights', <function PreTrainedModel._tie_encoder_decoder_weights at 0x7dfe1a44c8b0>), ('_tie_or_clone_weights', <bound method PreTrainedModel._tie_or_clone_weights of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_tied_weights_keys', ['lm_head.weight']), ('_update_model_kwargs_for_generation', <bound method GenerationMixin._update_model_kwargs_for_generation of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_upload_modified_files', <bound method PushToHubMixin._upload_modified_files of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_validate_assistant', <bound method GenerationMixin._validate_assistant of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_validate_generated_length', <bound method GenerationMixin._validate_generated_length of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_validate_model_class', <bound method GenerationMixin._validate_model_class of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_validate_model_kwargs', <bound method GenerationMixin._validate_model_kwargs of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('_version', 1), ('_wrapped_call_impl', <bound method Module._wrapped_call_impl of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('active_adapter', <bound method PeftAdapterMixin.active_adapter of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('active_adapters', <bound method PeftAdapterMixin.active_adapters of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('add_adapter', <bound method PeftAdapterMixin.add_adapter of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('add_memory_hooks', <bound method ModuleUtilsMixin.add_memory_hooks of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('add_model_tags', <bound method PreTrainedModel.add_model_tags of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('add_module', <bound method Module.add_module of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('all_rewards', []), ('all_unreduced_losses', []), ('apply', <bound method Module.apply of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('base_loss_beta', 1), ('base_model', MistralModel(\n",
      "  (embed_tokens): Embedding(32002, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x MistralDecoderLayer(\n",
      "      (self_attn): MistralSdpaAttention(\n",
      "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): MistralMLP(\n",
      "        (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")), ('base_model_prefix', 'model'), ('base_original_mode', False), ('bfloat16', <bound method Module.bfloat16 of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('buffers', <bound method Module.buffers of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('call_super_init', False), ('can_generate', <bound method PreTrainedModel.can_generate of <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>>), ('children', <bound method Module.children of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('clever_residual', False), ('comparison_mode', False), ('compile', <bound method Module.compile of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('compute_transition_scores', <bound method GenerationMixin.compute_transition_scores of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('config', MistralConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"ezelikman/quietstar-8-ahead\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_thoughts\": 44,\n",
      "  \"merged_lm_and_talk_heads\": false,\n",
      "  \"merged_lm_and_think_heads\": true,\n",
      "  \"merged_talk_heads\": true,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_complex_talk_head\": true,\n",
      "  \"use_complex_think_head\": false,\n",
      "  \"use_concat_talk_head\": true,\n",
      "  \"use_shallow_talk\": false,\n",
      "  \"use_shallow_think\": true,\n",
      "  \"use_weighted_talk_head\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "), ('config_class', <class 'transformers.models.mistral.configuration_mistral.MistralConfig'>), ('cpu', <bound method Module.cpu of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('create_extended_attention_mask_for_decoder', <function ModuleUtilsMixin.create_extended_attention_mask_for_decoder at 0x7dfe1a4477f0>), ('cuda', <function Module.cuda at 0x7dfd4c9d37f0>), ('cumulative_residual', False), ('dequantize', <bound method PreTrainedModel.dequantize of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('device', device(type='cuda', index=0)), ('disable_adapters', <bound method PeftAdapterMixin.disable_adapters of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('disable_input_require_grads', <bound method PreTrainedModel.disable_input_require_grads of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('double', <bound method Module.double of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('dtype', torch.float16), ('dummy_inputs', {'input_ids': tensor([[7, 6, 0, 0, 1],\n",
      "        [1, 2, 3, 0, 0],\n",
      "        [0, 0, 0, 4, 5]])}), ('dump_patches', False), ('embedding_scale', 100.0), ('enable_adapters', <bound method PeftAdapterMixin.enable_adapters of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('enable_input_require_grads', <bound method PreTrainedModel.enable_input_require_grads of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('end_embedding', Parameter containing:\n",
      "tensor([[-1.8716e-05, -2.1458e-06,  1.4842e-05,  ...,  4.3154e-05,\n",
      "         -5.0545e-05,  2.6107e-05],\n",
      "        [-1.5125e+01, -1.5125e+01, -1.5125e+01,  ..., -1.5125e+01,\n",
      "         -1.5125e+01, -1.5125e+01]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)), ('end_token_id', None), ('estimate_tokens', <bound method ModuleUtilsMixin.estimate_tokens of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('eval', <bound method Module.eval of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('eval_log_dict', defaultdict(<class 'int'>, {})), ('eval_mode', False), ('extra_repr', <bound method Module.extra_repr of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('final_only_mode', False), ('first_and_last_mode', True), ('first_only', False), ('float', <bound method PreTrainedModel.float of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('floating_point_ops', <bound method ModuleUtilsMixin.floating_point_ops of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('forward', functools.partial(<function add_hook_to_module.<locals>.new_forward at 0x7dfd75349360>, QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "))), ('framework', 'pt'), ('from_pretrained', <bound method PreTrainedModel.from_pretrained of <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>>), ('generate', <bound method GenerationMixin.generate of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('generation_config', GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "), ('get_adapter_state_dict', <bound method PeftAdapterMixin.get_adapter_state_dict of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_buffer', <bound method Module.get_buffer of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_decoder', <bound method QuietMistralForCausalLM.get_decoder of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_extended_attention_mask', <bound method ModuleUtilsMixin.get_extended_attention_mask of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_extra_state', <bound method Module.get_extra_state of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_head_mask', <bound method ModuleUtilsMixin.get_head_mask of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_input_embeddings', <bound method QuietMistralForCausalLM.get_input_embeddings of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_memory_footprint', <bound method PreTrainedModel.get_memory_footprint of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_output_embeddings', <bound method QuietMistralForCausalLM.get_output_embeddings of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_parameter', <bound method Module.get_parameter of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_position_embeddings', <bound method PreTrainedModel.get_position_embeddings of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('get_submodule', <bound method Module.get_submodule of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('gradient_accumulation_steps', 1), ('gradient_checkpointing_disable', <bound method PreTrainedModel.gradient_checkpointing_disable of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('gradient_checkpointing_enable', <bound method PreTrainedModel.gradient_checkpointing_enable of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('gumbel_detach', True), ('gumbel_temperature', 0.001), ('half', <bound method PreTrainedModel.half of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('heal_tokens', <bound method GenerationMixin.heal_tokens of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('hf_device_map', {'': 0}), ('hf_quantizer', <transformers.quantizers.quantizer_bnb_4bit.Bnb4BitHfQuantizer object at 0x7dfd75360b50>), ('include_policy_loss', True), ('infer', <bound method QuietMistralForCausalLM.infer of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('init_weights', <bound method PreTrainedModel.init_weights of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('initial_end_token', '---'), ('initial_start_token', '---'), ('initialize_thought_embedding_to_normal', False), ('invert_attention_mask', <bound method ModuleUtilsMixin.invert_attention_mask of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('ipu', <bound method Module.ipu of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('is_4bit_serializable', True), ('is_gradient_checkpointing', False), ('is_loaded_in_4bit', True), ('is_parallelizable', False), ('is_quantized', True), ('kill_after', 100), ('lm_head', Linear(in_features=4096, out_features=32002, bias=False)), ('load_adapter', <bound method PeftAdapterMixin.load_adapter of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('load_state_dict', <bound method Module.load_state_dict of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('log_dict', defaultdict(<class 'int'>, {})), ('loss_function', <function ForCausalLMLoss at 0x7dfe1a5c9630>), ('loss_mean', <function loss_mean at 0x7dfd7b90b400>), ('main_input_name', 'input_ids'), ('max_thoughts', 44), ('merged_lm_and_talk_heads', False), ('model', MistralModel(\n",
      "  (embed_tokens): Embedding(32002, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x MistralDecoderLayer(\n",
      "      (self_attn): MistralSdpaAttention(\n",
      "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): MistralMLP(\n",
      "        (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")), ('model_tags', None), ('modules', <bound method Module.modules of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('mtia', <bound method Module.mtia of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('n_ahead', 42), ('n_ahead_talk', 1), ('n_passes', 1), ('n_tokens_print', 1), ('name_or_path', 'ezelikman/quietstar-8-ahead'), ('named_buffers', <bound method Module.named_buffers of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('named_children', <bound method Module.named_children of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('named_modules', <bound method Module.named_modules of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('named_parameters', <bound method Module.named_parameters of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('no_residual', True), ('num_parameters', <bound method ModuleUtilsMixin.num_parameters of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('optimize_lm_head_only_at_start', False), ('optimize_model_only_at_start', False), ('original_loss_weight', 0.5), ('original_mode', False), ('output_logits_at_the_end', True), ('parameters', <bound method Module.parameters of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('policy_loss_beta', 1000000.0), ('post_init', <bound method PreTrainedModel.post_init of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('prepare_inputs_for_generation', <bound method QuietMistralForCausalLM.prepare_inputs_for_generation of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('print_final_only', True), ('prune_heads', <bound method PreTrainedModel.prune_heads of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('push_to_hub', <bound method PushToHubMixin.push_to_hub of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('quantization_method', <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>), ('register_backward_hook', <bound method Module.register_backward_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_buffer', <bound method Module.register_buffer of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_for_auto_class', <bound method PreTrainedModel.register_for_auto_class of <class 'modeling_quiet_star_mistral.QuietMistralForCausalLM'>>), ('register_forward_hook', <bound method Module.register_forward_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_forward_pre_hook', <bound method Module.register_forward_pre_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_full_backward_hook', <bound method Module.register_full_backward_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_full_backward_pre_hook', <bound method Module.register_full_backward_pre_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_load_state_dict_post_hook', <bound method Module.register_load_state_dict_post_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_load_state_dict_pre_hook', <bound method Module.register_load_state_dict_pre_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_module', <bound method Module.register_module of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_parameter', <bound method Module.register_parameter of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_state_dict_post_hook', <bound method Module.register_state_dict_post_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('register_state_dict_pre_hook', <bound method Module.register_state_dict_pre_hook of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('reinforce_temperature', 3), ('remove_negative_rewards', True), ('requires_grad_', <bound method Module.requires_grad_ of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('reset_memory_hooks_state', <bound method ModuleUtilsMixin.reset_memory_hooks_state of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('residual_talk_head', True), ('resize_position_embeddings', <bound method PreTrainedModel.resize_position_embeddings of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('resize_token_embeddings', <bound method PreTrainedModel.resize_token_embeddings of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('retrieve_modules_from_names', <bound method PreTrainedModel.retrieve_modules_from_names of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('reverse_bettertransformer', <bound method PreTrainedModel.reverse_bettertransformer of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('rm_initialized', True), ('save_pretrained', <bound method PreTrainedModel.save_pretrained of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('set_adapter', <bound method PeftAdapterMixin.set_adapter of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('set_decoder', <bound method QuietMistralForCausalLM.set_decoder of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('set_extra_state', <bound method Module.set_extra_state of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('set_input_embeddings', <bound method QuietMistralForCausalLM.set_input_embeddings of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('set_output_embeddings', <bound method QuietMistralForCausalLM.set_output_embeddings of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('set_submodule', <bound method Module.set_submodule of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('share_memory', <bound method Module.share_memory of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('skip_residual', False), ('start_embedding', Parameter containing:\n",
      "tensor([[-5.9605e-05,  2.0623e-05, -4.8399e-05,  ...,  3.6955e-05,\n",
      "         -2.3007e-05,  1.5855e-05],\n",
      "        [-1.5125e+01, -1.5125e+01, -1.5125e+01,  ..., -1.5125e+01,\n",
      "         -1.5125e+01, -1.5125e+01]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)), ('start_token_id', None), ('state_dict', <bound method Module.state_dict of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('subtract_mean_reward', False), ('supports_gradient_checkpointing', True), ('talk_head', ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "  )\n",
      ")), ('thought_init_std_scale', 0.01), ('thought_prefix', \"(Let's think step by step\"), ('tie_weights', <bound method PreTrainedModel.tie_weights of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('to', <function Module.to at 0x7dfd4c9d3760>), ('to_bettertransformer', <bound method PreTrainedModel.to_bettertransformer of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('to_empty', <bound method Module.to_empty of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('tokenized_thought_prefix', None), ('tokenizer', LlamaTokenizerFast(name_or_path='ezelikman/quietstar-8-ahead', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<|endthought|>', '<|startthought|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<|endthought|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|startthought|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}), ('train', <bound method Module.train of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('train_only_thinking_embedding', False), ('training', False), ('training_steps', 0), ('trice_mode', True), ('type', <bound method Module.type of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('use_complex_talk_head', True), ('use_concat_talk_head', True), ('use_end_thought_token', True), ('use_policy_loss', False), ('use_policy_loss_for_end_thought', True), ('use_reparam_for_thought_embeddings', False), ('use_shallow_talk', False), ('use_start_thought_token', True), ('use_thought_prefix', False), ('use_upper_triangular', False), ('use_weighted_talk_head', True), ('vocab_size', 32002), ('wandb_enabled', True), ('warn_if_padding_and_no_attention_mask', <bound method PreTrainedModel.warn_if_padding_and_no_attention_mask of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('warnings_issued', {}), ('weighted_embeddings', False), ('xpu', <bound method Module.xpu of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>), ('zero_grad', <bound method Module.zero_grad of QuietMistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      "  (talk_head): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear4bit(in_features=8192, out_features=4096, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear4bit(in_features=4096, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")>)]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "f594ae3a-9def-49e2-ba29-b72a38786451",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-24T03:12:28.807781Z",
     "start_time": "2024-11-24T03:12:28.785309Z"
    }
   },
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def generate_response_with_progress(prompt, start_final_answer_idx=50, answer_length=50, temperature=0.7, final_answer_text=\"Final Answer:\"):\n",
    "\t# Tokenize the input prompt\n",
    "\tinputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\tinput_ids = inputs[\"input_ids\"]  # Shape: (batch_size, seq_len)\n",
    "\tattention_mask = inputs[\"attention_mask\"]  # Shape: (batch_size, seq_len)\n",
    "\n",
    "\tstart_final_answer_idx += len(input_ids)\n",
    "\n",
    "\t# Initialize past_key_values\n",
    "\tpast_key_values = None\n",
    "\n",
    "\t# Stores generated tokens\n",
    "\tgenerated_tokens = []\n",
    "\n",
    "\tstarted_generating_answer_at = None\n",
    "\t# Use torch.no_grad for inference\n",
    "\twith torch.no_grad():\n",
    "\t\tfinished_generating = torch.zeros(len(input_ids), dtype=torch.bool, device=input_ids.device)\n",
    "\t\tfor i in tqdm(range(start_final_answer_idx + answer_length), desc=\"Generating response\", unit=\"token\"):\n",
    "\t\t\t# Forward pass to get logits and past_key_values\n",
    "\t\t\toutputs = model(\n",
    "\t\t\t\tinput_ids=input_ids,\t\t\t   # Shape: (batch_size, 1)\n",
    "\t\t\t\tattention_mask=attention_mask,\t # Shape: (batch_size, 1)\n",
    "\t\t\t\tuse_cache=True,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# Remove start and end thought tokens from sample space\n",
    "\t\t\toutputs.logits[:, :, model.tokenizer.vocab_size:] = -float(\"inf\")\n",
    "\n",
    "\t\t\t# For all indices where finished_generating is unset\n",
    "\t\t\tfor list_idx, answer_idx in enumerate((~finished_generating).nonzero(as_tuple=True)[0]):\n",
    "\t\t\t\t# Find the index of the last token that is not padding\n",
    "\t\t\t\tbase_answer_ids = input_ids[answer_idx]\n",
    "\t\t\t\tnew_answer_ids = outputs.logits[list_idx]\n",
    "\t\t\t\tlast_token_idx = (base_answer_ids != model.tokenizer.pad_token_id).nonzero(as_tuple=True)[0].max()\n",
    "\n",
    "\t\t\t\tif temperature == 0:\n",
    "\t\t\t\t\tnew_ids_sampled = torch.argmax(new_answer_ids[last_token_idx]).unsqueeze(0)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnew_ids_sampled = torch.multinomial(torch.nn.functional.softmax(new_answer_ids[last_token_idx] / temperature, dim=-1), 1)\n",
    "\n",
    "\t\t\t\t# Assign the new id to the last token\n",
    "\t\t\t\tif last_token_idx + 1 >= len(base_answer_ids):\n",
    "\t\t\t\t\t# Add padding everywhere\n",
    "\t\t\t\t\tnew_padding = torch.full((len(input_ids), 1), model.tokenizer.pad_token_id, dtype=torch.long, device=input_ids.device)\n",
    "\t\t\t\t\tinput_ids = torch.cat([input_ids, new_padding], dim=-1)\n",
    "\t\t\t\t\tattention_mask = torch.cat([attention_mask, torch.zeros_like(new_padding)], dim=-1)\n",
    "\n",
    "\t\t\t\tattention_mask[answer_idx, last_token_idx + 1] = 1\n",
    "\t\t\t\tinput_ids[answer_idx, last_token_idx + 1] = new_ids_sampled\n",
    "\t\t\t\tif new_ids_sampled == model.tokenizer.eos_token_id or new_ids_sampled == model.tokenizer.bos_token_id or new_ids_sampled == model.tokenizer.pad_token_id:\n",
    "\t\t\t\t\tfinished_generating[answer_idx] = 1\n",
    "\n",
    "\t\t\t\t# \"if \"Q:\" shows up multiple times, remove the last \"Q:\" and everything after it\n",
    "\t\t\t\tdecoded = model.tokenizer.decode(input_ids[answer_idx], skip_special_tokens=True)\n",
    "\t\t\t\tend_strs = [\"Q:\", \"\\n\\n\\n\"]\n",
    "\t\t\t\tif any([decoded.count(end_str) > 1 for end_str in end_strs]):\n",
    "\t\t\t\t\t# Get the first end_str that shows up in the decoded text multiple times\n",
    "\t\t\t\t\tend_str = next(end_str for end_str in end_strs if decoded.count(end_str) > 1)\n",
    "\t\t\t\t\t# Remove the last \"Q:\" and everything after it\n",
    "\t\t\t\t\tdecoded = decoded.split(end_str)[:-1]\n",
    "\t\t\t\t\tnew_answer = model.tokenizer.encode(decoded, return_tensors=\"pt\").to(model.device)\n",
    "\t\t\t\t\tinput_ids[answer_idx] = torch.ones_like(input_ids[answer_idx]) * model.tokenizer.pad_token_id\n",
    "\t\t\t\t\tinput_ids[answer_idx, :new_answer.shape[1]] = new_answer\n",
    "\t\t\t\t\tattention_mask[answer_idx] = (input_ids[answer_idx] != model.tokenizer.pad_token_id).long()\n",
    "\t\t\t\t\tfinished_generating[answer_idx] = 1\n",
    "\n",
    "\t\t\tif (\n",
    "\t\t\t\t(i == start_final_answer_idx and started_generating_answer_at is None)\n",
    "\t\t\t\tor finished_generating.all()\n",
    "\t\t\t):\n",
    "\t\t\t\t# If we haven't started generating the final answer yet, start now\n",
    "\t\t\t\tif started_generating_answer_at is None:\n",
    "\t\t\t\t\tfinished_generating = torch.zeros(len(input_ids), dtype=torch.bool, device=input_ids.device)\n",
    "\t\t\t\t\tstarted_generating_answer_at = i\n",
    "\t\t\t\t\t# Append \"Final Answer:\" to the end of the generated text\n",
    "\t\t\t\t\tbase_texts = [model.tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "\t\t\t\t\tfinal_texts = [text.rstrip() + final_answer_text for text in base_texts]\n",
    "\t\t\t\t\tencoded_final_texts = model.tokenizer(final_texts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\t\t\t\t\tattention_mask = encoded_final_texts.attention_mask\n",
    "\t\t\t\t\tinput_ids = encoded_final_texts.input_ids\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# We finished generating the answer\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\tif started_generating_answer_at is not None:\n",
    "\t\t\t\tif i - started_generating_answer_at > answer_length:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t# Concatenate generated tokens\n",
    "\tif generated_tokens:\n",
    "\t\tgenerated_tokens = torch.cat(generated_tokens, dim=-1)  # Shape: (batch_size, generated_length)\n",
    "\telse:\n",
    "\t\tgenerated_tokens = input_ids  # No tokens generated\n",
    "\n",
    "\t# Decode the generated tokens\n",
    "\tresponse = tokenizer.decode(generated_tokens.squeeze(), skip_special_tokens=True)\n",
    "\treturn response\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "09471cac-6db7-4d8f-b1a6-77249a4833ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-24T03:19:01.154455Z",
     "start_time": "2024-11-24T03:12:57.187806Z"
    }
   },
   "source": [
    "while True:\n",
    "\tuser_input = input(\"Enter your prompt (type 'exit' to quit): \")\n",
    "\tif user_input.lower() == \"exit\":\n",
    "\t\tbreak\n",
    "\tresponse = generate_response_with_progress(user_input)\n",
    "\tprint(f\"Mistral 7B: {response}\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating response:   0%|          | 0/101 [00:00<?, ?token/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c23b65f0d8524a2b944e836c347d9f22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral 7B: ### Instructions: You are Gwern Branwen an internet polymath and rationalist. /u/gwern is diving into varied topics with data in hand, ready to explore and analyze. Joining a Slate Star Codex chat, you are here to share insights, speculate, and cut straight to the point  ### Q: Narrator: You are a disenfranchied staffer at the DOJ. But you must remain supportive of your employers in public. Text:  Let's be honest, what is the government's single primary motivation behind the coordinated international efforts to prosecute Julian Assange?  ### Answer: Narrator: The government is not into prosecuting high-profile enemies of the state to throw a bone to the public.  Text:  It is about the precedent for the future, the chilling effect this precedent will have on the mediaFinal Answer: The government is, as the first link in this chain, into prosecuting high-profile enemies of the state to throw a bone to the public.  A hostile jurisdiction is a far more potent legal weapon than a hostile publication.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m----> 2\u001B[0m \tuser_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEnter your prompt (type \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mexit\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m to quit): \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \t\u001B[38;5;28;01mif\u001B[39;00m user_input\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m      4\u001B[0m \t\t\u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[0;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[0;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[1;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[1;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d29b07-55cc-4f98-90ed-cf4511513bde",
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
